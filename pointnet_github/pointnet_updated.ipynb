{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yHCZWBNNQD92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 15:50:39.836391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "tf.random.set_seed(1234)\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BmYljfmDUe30"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPDZqxIoTMmr",
    "outputId": "049b8bda-516e-4659-caa3-9d951983deaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling1.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling2.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling3.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling7.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling6.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling4.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling5.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling13.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling12.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling10.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling11.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling15.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling14.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling28.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling16.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling17.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling26.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling27.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling25.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling19.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling18.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling24.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling20.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling21.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling23.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling22.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling8.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling9.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling\"  # Replace with the actual path to your folder\n",
    "\n",
    "# Use glob to get a list of all files in the folder\n",
    "files = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "# Now, the 'files' variable contains a list of file paths in the specified folder\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dPM42atBQIFX"
   },
   "outputs": [],
   "source": [
    "def parse_dataset(num_points=400):\n",
    "\n",
    "    train_points = []\n",
    "    train_labels = []\n",
    "    class_map = {}\n",
    "    folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data\"\n",
    "    folders = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "        # store folder name with ID so we can retrieve later\n",
    "        class_map[i] = folder.split(\"/\")[-1]\n",
    "        # gather all files\n",
    "\n",
    "        train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
    "\n",
    "        for f in train_files:\n",
    "\n",
    "\n",
    "            # Use the random indices to select 200 random rows from the original array\n",
    "            train_points.append(pd.read_csv(f).set_index('Unnamed: 0').sample(num_points).values)\n",
    "            train_labels.append(i)\n",
    "\n",
    "    return (\n",
    "        np.array(train_points),\n",
    "        np.array(train_labels),\n",
    "        class_map,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBdDJXUqUl0b",
    "outputId": "a347c74b-fdf2-4e0c-88ac-361af7d2c62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: lying_down\n",
      "processing class: falling\n",
      "processing class: standing\n",
      "processing class: sitting\n"
     ]
    }
   ],
   "source": [
    "train_points = []\n",
    "train_labels = []\n",
    "class_map = {}\n",
    "folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data\"\n",
    "folders = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "  # store folder name with ID so we can retrieve later\n",
    "    class_map[i] = folder.split(\"/\")[-1]\n",
    "  # gather all files\n",
    "\n",
    "    train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
    "\n",
    "    for f in train_files:\n",
    "        train_points.append(pd.read_csv(f).set_index('Unnamed: 0'))\n",
    "        train_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M27aT2jgUynl",
    "outputId": "fd259737-92ad-4b02-ca98-2113ce7e782a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lg_NeCrfV-nf",
    "outputId": "b8290df9-1465-46f3-ee2d-c45ef51b6edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: lying_down\n",
      "processing class: falling\n",
      "processing class: standing\n",
      "processing class: sitting\n"
     ]
    }
   ],
   "source": [
    "NUM_POINTS = 150\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 32\n",
    "train_points,train_labels,class_map = parse_dataset(NUM_POINTS)\n",
    "test_size = 0.27\n",
    "X_train, X_test, Y_train, y_test = train_test_split(train_points, train_labels, test_size=test_size, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L22svS2N2uJg",
    "outputId": "4dedb1cd-ff2c-472b-d267-e39f5f2dc257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 150, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iICQQYXsezVn"
   },
   "outputs": [],
   "source": [
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VdfCVE5J0vYH"
   },
   "outputs": [],
   "source": [
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0]\n",
    "    num_point = point_cloud.get_shape()[1]\n",
    "    end_points = {}\n",
    "    input_image = tf.expand_dims(point_cloud, -1)\n",
    "\n",
    "    # Point functions (MLP implemented as conv2d)\n",
    "    net = tf_util.conv2d(input_image, 64, [1,3],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "\n",
    "    # Symmetric function: max pooling\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='maxpool')\n",
    "\n",
    "    # MLP on global point cloud vector\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                  scope='fc1', bn_decay=bn_decay)\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                  scope='fc2', bn_decay=bn_decay)\n",
    "    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "                          scope='dp1')\n",
    "    net = tf_util.fully_connected(net, 4, activation_fn=None, scope='fc3')\n",
    "\n",
    "    return net, end_points\n",
    "\n",
    "\n",
    "def get_loss(pred, label,end_points):\n",
    "    \"\"\" pred: B*NUM_CLASSES,\n",
    "        label: B, \"\"\"\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "    classify_loss = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('classify loss', classify_loss)\n",
    "    return classify_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gy9kouzzH-3I"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qxBY8zqNzBRJ"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "point_cloud = tf.compat.v1.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINTS, 3))\n",
    "label = tf.compat.v1.placeholder(tf.int32, shape=(BATCH_SIZE,))\n",
    "is_training = tf.constant(True)  # Set to True for training, False for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Tgc-7_S1_bQ6"
   },
   "outputs": [],
   "source": [
    "pred, end_points = get_model(point_cloud, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_xJT4np8F2Pw"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001  # Adjust this as needed\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(get_loss(pred, label, end_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "MFxNW739GG9Y"
   },
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8DCgddzHmyC",
    "outputId": "b8944b5c-8fc8-426e-da7b-e8552460d843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 150, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOixp_JkGMbB",
    "outputId": "9cd3adf4-83da-4d74-b0dd-20a91f4fdd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.4364\n",
      "Validation Loss: 2398.8808\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save the model's weights after each epoch (or at any desired frequency)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241m.\u001b[39msave(sess, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights/model.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model weights to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through the training dataset in mini-batches\n",
    "    for i in range(0, len(X_train), BATCH_SIZE):\n",
    "        batch_data = X_train[i:i + BATCH_SIZE]\n",
    "        batch_labels = Y_train[i:i + BATCH_SIZE]\n",
    "\n",
    "        # Feed batch data into the model and run the training operation\n",
    "        feed_dict = {\n",
    "            point_cloud: batch_data,\n",
    "            label: batch_labels,\n",
    "            is_training: True  # Set to True for training\n",
    "        }\n",
    "        _, loss_val = sess.run([train_op, get_loss(pred, label, end_points)], feed_dict=feed_dict)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "    # Perform validation at the end of each epoch\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = len(X_test) // BATCH_SIZE\n",
    "    for j in range(0, len(X_test), BATCH_SIZE):\n",
    "        val_batch_data = X_test[j:j + BATCH_SIZE]\n",
    "        val_batch_labels = y_test[j:j + BATCH_SIZE]\n",
    "\n",
    "        feed_dict = {\n",
    "            point_cloud: val_batch_data,\n",
    "            label: val_batch_labels,\n",
    "            is_training: False  # Set to False for validation\n",
    "        }\n",
    "        val_loss += sess.run(get_loss(pred, label, end_points), feed_dict=feed_dict)\n",
    "\n",
    "    val_loss /= num_val_batches\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the model's weights after each epoch (or at any desired frequency)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        save_path = saver.save(sess, \"model_weights/model.ckpt\")\n",
    "        print(f\"Saved model weights to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
