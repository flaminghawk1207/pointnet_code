{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yHCZWBNNQD92"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BmYljfmDUe30"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPDZqxIoTMmr",
    "outputId": "049b8bda-516e-4659-caa3-9d951983deaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling1.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling2.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling3.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling7.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling6.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling4.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling5.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling13.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling12.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling10.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling11.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling15.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling14.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling28.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling16.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling17.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling26.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling27.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling25.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling19.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling18.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling24.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling20.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling21.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling23.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling22.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling8.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling9.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling\"  # Replace with the actual path to your folder\n",
    "\n",
    "# Use glob to get a list of all files in the folder\n",
    "files = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "# Now, the 'files' variable contains a list of file paths in the specified folder\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dPM42atBQIFX"
   },
   "outputs": [],
   "source": [
    "def parse_dataset(num_points=400):\n",
    "\n",
    "    train_points = []\n",
    "    train_labels = []\n",
    "    class_map = {}\n",
    "    folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data\"\n",
    "    folders = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "        # store folder name with ID so we can retrieve later\n",
    "        class_map[i] = folder.split(\"/\")[-1]\n",
    "        # gather all files\n",
    "\n",
    "        train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
    "\n",
    "        for f in train_files:\n",
    "\n",
    "\n",
    "            # Use the random indices to select 200 random rows from the original array\n",
    "            train_points.append(pd.read_csv(f).set_index('Unnamed: 0').sample(num_points).values)\n",
    "            train_labels.append(i)\n",
    "\n",
    "    return (\n",
    "        np.array(train_points),\n",
    "        np.array(train_labels),\n",
    "        class_map,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBdDJXUqUl0b",
    "outputId": "a347c74b-fdf2-4e0c-88ac-361af7d2c62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: lying_down\n",
      "processing class: falling\n",
      "processing class: standing\n",
      "processing class: sitting\n"
     ]
    }
   ],
   "source": [
    "train_points = []\n",
    "train_labels = []\n",
    "class_map = {}\n",
    "folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data\"\n",
    "folders = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "  # store folder name with ID so we can retrieve later\n",
    "    class_map[i] = folder.split(\"/\")[-1]\n",
    "  # gather all files\n",
    "\n",
    "    train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
    "\n",
    "    for f in train_files:\n",
    "        train_points.append(pd.read_csv(f).set_index('Unnamed: 0'))\n",
    "        train_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M27aT2jgUynl",
    "outputId": "fd259737-92ad-4b02-ca98-2113ce7e782a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lg_NeCrfV-nf",
    "outputId": "b8290df9-1465-46f3-ee2d-c45ef51b6edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: lying_down\n",
      "processing class: falling\n",
      "processing class: standing\n",
      "processing class: sitting\n"
     ]
    }
   ],
   "source": [
    "NUM_POINTS = 150\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 32\n",
    "train_points,train_labels,class_map = parse_dataset(NUM_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L22svS2N2uJg",
    "outputId": "4dedb1cd-ff2c-472b-d267-e39f5f2dc257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 150, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iICQQYXsezVn"
   },
   "outputs": [],
   "source": [
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VdfCVE5J0vYH"
   },
   "outputs": [],
   "source": [
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0]\n",
    "    num_point = point_cloud.get_shape()[1]\n",
    "    end_points = {}\n",
    "    input_image = tf.expand_dims(point_cloud, -1)\n",
    "\n",
    "    # Point functions (MLP implemented as conv2d)\n",
    "    net = tf_util.conv2d(input_image, 64, [1,3],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "\n",
    "    # Symmetric function: max pooling\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='maxpool')\n",
    "\n",
    "    # MLP on global point cloud vector\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                  scope='fc1', bn_decay=bn_decay)\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                  scope='fc2', bn_decay=bn_decay)\n",
    "    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "                          scope='dp1')\n",
    "    net = tf_util.fully_connected(net, 4, activation_fn=None, scope='fc3')\n",
    "\n",
    "    return net, end_points\n",
    "\n",
    "\n",
    "def get_loss(pred, label,end_points):\n",
    "    \"\"\" pred: B*NUM_CLASSES,\n",
    "        label: B, \"\"\"\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "    classify_loss = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('classify loss', classify_loss)\n",
    "    return classify_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "gy9kouzzH-3I"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qxBY8zqNzBRJ"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "point_cloud = tf.compat.v1.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINTS, 3))\n",
    "label = tf.compat.v1.placeholder(tf.int32, shape=(BATCH_SIZE,))\n",
    "is_training = tf.constant(True)  # Set to True for training, False for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Tgc-7_S1_bQ6"
   },
   "outputs": [],
   "source": [
    "pred, end_points = get_model(point_cloud, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_xJT4np8F2Pw"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001  # Adjust this as needed\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(get_loss(pred, label, end_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "MFxNW739GG9Y"
   },
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8DCgddzHmyC",
    "outputId": "b8944b5c-8fc8-426e-da7b-e8552460d843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 150, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOixp_JkGMbB",
    "outputId": "9cd3adf4-83da-4d74-b0dd-20a91f4fdd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.7770\n",
      "Saved model weights to model_weights/model.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Feed batch data into the model and run the training operation\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     feed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m         point_cloud: batch_data,\n\u001b[1;32m     13\u001b[0m         label: batch_labels,\n\u001b[1;32m     14\u001b[0m         is_training: \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set to True for training\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[0;32m---> 16\u001b[0m     _, loss_val \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Save the model's weights after each epoch (or at any desired frequency)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/client/session.py:1130\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1128\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten_dict_items(feed_dict)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feed, feed_val \u001b[38;5;129;01min\u001b[39;00m feed_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1130\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m subfeed, subfeed_val \u001b[38;5;129;01min\u001b[39;00m _feed_fn(feed, feed_val):\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m       subfeed_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mas_graph_element(\n\u001b[1;32m   1133\u001b[0m           subfeed, allow_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_operation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through the dataset in mini-batches\n",
    "    for i in range(0, len(train_points), BATCH_SIZE):\n",
    "        batch_data = train_points[i:i + BATCH_SIZE]\n",
    "        batch_labels = train_labels[i:i + BATCH_SIZE]\n",
    "\n",
    "        # Feed batch data into the model and run the training operation\n",
    "        feed_dict = {\n",
    "            point_cloud: batch_data,\n",
    "            label: batch_labels,\n",
    "            is_training: True  # Set to True for training\n",
    "        }\n",
    "        _, loss_val = sess.run([train_op, get_loss(pred, label, end_points)], feed_dict=feed_dict)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "    # Save the model's weights after each epoch (or at any desired frequency)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        save_path = saver.save(sess, \"model_weights/model.ckpt\")\n",
    "        print(f\"Saved model weights to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
