{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cnsfPyGP40l",
        "outputId": "b7f0a22d-1f58-4bf5-c1cd-6b264efe8a22"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "yHCZWBNNQD92"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BmYljfmDUe30"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling\"  # Replace with the actual path to your folder\n",
        "\n",
        "# Use glob to get a list of all files in the folder\n",
        "files = glob.glob(folder_path + \"/*\")\n",
        "\n",
        "# Now, the 'files' variable contains a list of file paths in the specified folder\n",
        "print(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPDZqxIoTMmr",
        "outputId": "049b8bda-516e-4659-caa3-9d951983deaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling13.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling5.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling12.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling16.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling14.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling6.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling4.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling11.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling2.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling10.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling9.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling7.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling3.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling15.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling8.csv', '/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main/falling/falling1.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dataset(num_points=400):\n",
        "\n",
        "    train_points = []\n",
        "    train_labels = []\n",
        "    class_map = {}\n",
        "    folder_path = \"/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main\"\n",
        "    folders = glob.glob(folder_path + \"/*\")\n",
        "\n",
        "    for i, folder in enumerate(folders):\n",
        "        print(\"processing class: {}\".format(os.path.basename(folder)))\n",
        "        # store folder name with ID so we can retrieve later\n",
        "        class_map[i] = folder.split(\"/\")[-1]\n",
        "        # gather all files\n",
        "\n",
        "        train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
        "\n",
        "        for f in train_files:\n",
        "\n",
        "\n",
        "            # Use the random indices to select 200 random rows from the original array\n",
        "            train_points.append(pd.read_csv(f).set_index('Unnamed: 0').sample(num_points).values)\n",
        "            train_labels.append(i)\n",
        "\n",
        "    return (\n",
        "        np.array(train_points),\n",
        "        np.array(train_labels),\n",
        "        class_map,\n",
        "    )"
      ],
      "metadata": {
        "id": "dPM42atBQIFX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_points = []\n",
        "train_labels = []\n",
        "class_map = {}\n",
        "folder_path = \"/content/drive/MyDrive/lidar_fall_detection/annotation/annotation_main\"\n",
        "folders = glob.glob(folder_path + \"/*\")\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "  print(\"processing class: {}\".format(os.path.basename(folder)))\n",
        "  # store folder name with ID so we can retrieve later\n",
        "  class_map[i] = folder.split(\"/\")[-1]\n",
        "  # gather all files\n",
        "\n",
        "  train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
        "\n",
        "  for f in train_files:\n",
        "    train_points.append(pd.read_csv(f).set_index('Unnamed: 0'))\n",
        "    train_labels.append(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBdDJXUqUl0b",
        "outputId": "a347c74b-fdf2-4e0c-88ac-361af7d2c62e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing class: falling\n",
            "processing class: lying_down\n",
            "processing class: sitting\n",
            "processing class: standing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_points[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M27aT2jgUynl",
        "outputId": "fd259737-92ad-4b02-ca98-2113ce7e782a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(698, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_POINTS = 260\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 32\n",
        "train_points,train_labels,class_map = parse_dataset(NUM_POINTS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg_NeCrfV-nf",
        "outputId": "b8290df9-1465-46f3-ee2d-c45ef51b6edc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing class: falling\n",
            "processing class: lying_down\n",
            "processing class: sitting\n",
            "processing class: standing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_points.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L22svS2N2uJg",
        "outputId": "4dedb1cd-ff2c-472b-d267-e39f5f2dc257"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(296, 260, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/lidar_fall_detection')\n",
        "import tf_util"
      ],
      "metadata": {
        "id": "iICQQYXsezVn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(point_cloud, is_training, bn_decay=None):\n",
        "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
        "    batch_size = point_cloud.get_shape()[0]\n",
        "    num_point = point_cloud.get_shape()[1]\n",
        "    end_points = {}\n",
        "    input_image = tf.expand_dims(point_cloud, -1)\n",
        "\n",
        "    # Point functions (MLP implemented as conv2d)\n",
        "    net = tf_util.conv2d(input_image, 64, [1,3],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv1', bn_decay=bn_decay)\n",
        "    net = tf_util.conv2d(net, 64, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv2', bn_decay=bn_decay)\n",
        "    net = tf_util.conv2d(net, 64, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv3', bn_decay=bn_decay)\n",
        "    net = tf_util.conv2d(net, 128, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv4', bn_decay=bn_decay)\n",
        "    net = tf_util.conv2d(net, 1024, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv5', bn_decay=bn_decay)\n",
        "\n",
        "    # Symmetric function: max pooling\n",
        "    net = tf_util.max_pool2d(net, [num_point,1],\n",
        "                             padding='VALID', scope='maxpool')\n",
        "\n",
        "    # MLP on global point cloud vector\n",
        "    net = tf.reshape(net, [batch_size, -1])\n",
        "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
        "                                  scope='fc1', bn_decay=bn_decay)\n",
        "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
        "                                  scope='fc2', bn_decay=bn_decay)\n",
        "    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
        "                          scope='dp1')\n",
        "    net = tf_util.fully_connected(net, 4, activation_fn=None, scope='fc3')\n",
        "\n",
        "    return net, end_points\n",
        "\n",
        "\n",
        "def get_loss(pred, label,end_points):\n",
        "    \"\"\" pred: B*NUM_CLASSES,\n",
        "        label: B, \"\"\"\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
        "    classify_loss = tf.reduce_mean(loss)\n",
        "    tf.summary.scalar('classify loss', classify_loss)\n",
        "    return classify_loss"
      ],
      "metadata": {
        "id": "VdfCVE5J0vYH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 37"
      ],
      "metadata": {
        "id": "gy9kouzzH-3I"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "point_cloud = tf.compat.v1.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINTS, 3))\n",
        "label = tf.compat.v1.placeholder(tf.int32, shape=(BATCH_SIZE,))\n",
        "is_training = tf.constant(True)  # Set to True for training, False for inference\n"
      ],
      "metadata": {
        "id": "qxBY8zqNzBRJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred, end_points = get_model(point_cloud, is_training)"
      ],
      "metadata": {
        "id": "Tgc-7_S1_bQ6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001  # Adjust this as needed\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "train_op = optimizer.minimize(get_loss(pred, label, end_points))"
      ],
      "metadata": {
        "id": "_xJT4np8F2Pw"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.compat.v1.Session()\n",
        "sess.run(tf.compat.v1.global_variables_initializer())"
      ],
      "metadata": {
        "id": "MFxNW739GG9Y"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_points.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8DCgddzHmyC",
        "outputId": "b8944b5c-8fc8-426e-da7b-e8552460d843"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(296, 260, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=50\n",
        "for epoch in range(num_epochs):\n",
        "    # Loop through the dataset in mini-batches\n",
        "    for i in range(0, len(train_points), BATCH_SIZE):\n",
        "        batch_data = train_points[i:i+BATCH_SIZE]\n",
        "        batch_labels = train_labels[i:i+BATCH_SIZE]\n",
        "\n",
        "        #print(\"Batch data shape:\", batch_data.shape)\n",
        "        #print(\"Batch labels shape:\", batch_labels.shape)\n",
        "\n",
        "        # Feed batch data into the model and run the training operation\n",
        "        feed_dict = {\n",
        "            point_cloud: batch_data,\n",
        "            label: batch_labels,\n",
        "            is_training: True  # Set to True for training\n",
        "        }\n",
        "        _, loss_val = sess.run([train_op, get_loss(pred, label, end_points)], feed_dict=feed_dict)\n",
        "\n",
        "        # Print loss for monitoring\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} , Loss: {loss_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOixp_JkGMbB",
        "outputId": "9cd3adf4-83da-4d74-b0dd-20a91f4fdd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 , Loss: 2.1549\n",
            "Epoch 2/50 , Loss: 2.3441\n",
            "Epoch 3/50 , Loss: 2.6451\n",
            "Epoch 4/50 , Loss: 1.6587\n",
            "Epoch 5/50 , Loss: 1.3829\n",
            "Epoch 6/50 , Loss: 1.5203\n",
            "Epoch 7/50 , Loss: 1.1913\n",
            "Epoch 8/50 , Loss: 1.1506\n",
            "Epoch 9/50 , Loss: 1.5447\n",
            "Epoch 10/50 , Loss: 0.8114\n",
            "Epoch 11/50 , Loss: 0.9340\n",
            "Epoch 12/50 , Loss: 0.8116\n",
            "Epoch 13/50 , Loss: 0.6239\n",
            "Epoch 14/50 , Loss: 0.6227\n",
            "Epoch 15/50 , Loss: 0.5560\n",
            "Epoch 16/50 , Loss: 0.4463\n",
            "Epoch 17/50 , Loss: 0.7066\n",
            "Epoch 18/50 , Loss: 0.4898\n",
            "Epoch 19/50 , Loss: 0.7594\n"
          ]
        }
      ]
    }
  ]
}