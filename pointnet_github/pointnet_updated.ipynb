{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yHCZWBNNQD92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 19:29:31.799007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "tf.random.set_seed(1234)\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BmYljfmDUe30"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPDZqxIoTMmr",
    "outputId": "049b8bda-516e-4659-caa3-9d951983deaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling1.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling2.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling3.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling7.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling6.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling4.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling5.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling13.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling12.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling10.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling11.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling15.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling14.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling28.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling16.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling17.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling26.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling27.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling25.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling19.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling18.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling24.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling20.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling21.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling23.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling22.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling8.csv', '/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling/falling9.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data/falling\"  # Replace with the actual path to your folder\n",
    "\n",
    "# Use glob to get a list of all files in the folder\n",
    "files = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "# Now, the 'files' variable contains a list of file paths in the specified folder\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dPM42atBQIFX"
   },
   "outputs": [],
   "source": [
    "def parse_dataset(num_points=400):\n",
    "\n",
    "    train_points = []\n",
    "    train_labels = []\n",
    "    class_map = {}\n",
    "    folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data\"\n",
    "    folders = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "        # store folder name with ID so we can retrieve later\n",
    "        class_map[i] = folder.split(\"/\")[-1]\n",
    "        # gather all files\n",
    "\n",
    "        train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
    "\n",
    "        for f in train_files:\n",
    "\n",
    "\n",
    "            # Use the random indices to select 200 random rows from the original array\n",
    "            train_points.append(pd.read_csv(f).set_index('Unnamed: 0').sample(num_points).values)\n",
    "            train_labels.append(i)\n",
    "\n",
    "    return (\n",
    "        np.array(train_points),\n",
    "        np.array(train_labels),\n",
    "        class_map,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBdDJXUqUl0b",
    "outputId": "a347c74b-fdf2-4e0c-88ac-361af7d2c62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: lying_down\n",
      "processing class: falling\n",
      "processing class: standing\n",
      "processing class: sitting\n"
     ]
    }
   ],
   "source": [
    "train_points = []\n",
    "train_labels = []\n",
    "class_map = {}\n",
    "folder_path = \"/Users/anishayyagari/Documents/tester/pointnet implementation/data\"\n",
    "folders = glob.glob(folder_path + \"/*\")\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "  # store folder name with ID so we can retrieve later\n",
    "    class_map[i] = folder.split(\"/\")[-1]\n",
    "  # gather all files\n",
    "\n",
    "    train_files = glob.glob(os.path.join(folder_path,folder)+\"/*\")\n",
    "\n",
    "    for f in train_files:\n",
    "        train_points.append(pd.read_csv(f).set_index('Unnamed: 0'))\n",
    "        train_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M27aT2jgUynl",
    "outputId": "fd259737-92ad-4b02-ca98-2113ce7e782a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lg_NeCrfV-nf",
    "outputId": "b8290df9-1465-46f3-ee2d-c45ef51b6edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: lying_down\n",
      "processing class: falling\n",
      "processing class: standing\n",
      "processing class: sitting\n"
     ]
    }
   ],
   "source": [
    "NUM_POINTS = 150\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 32\n",
    "train_points,train_labels,class_map = parse_dataset(NUM_POINTS)\n",
    "test_size = 0.21\n",
    "X_train, X_test, Y_train, y_test = train_test_split(train_points, train_labels, test_size=test_size, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L22svS2N2uJg",
    "outputId": "4dedb1cd-ff2c-472b-d267-e39f5f2dc257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 150, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 150, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iICQQYXsezVn"
   },
   "outputs": [],
   "source": [
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VdfCVE5J0vYH"
   },
   "outputs": [],
   "source": [
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0]\n",
    "    num_point = point_cloud.get_shape()[1]\n",
    "    end_points = {}\n",
    "    input_image = tf.expand_dims(point_cloud, -1)\n",
    "\n",
    "    # Point functions (MLP implemented as conv2d)\n",
    "    net = tf_util.conv2d(input_image, 64, [1,3],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "\n",
    "    # Symmetric function: max pooling\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='maxpool')\n",
    "\n",
    "    # MLP on global point cloud vector\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                  scope='fc1', bn_decay=bn_decay)\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                  scope='fc2', bn_decay=bn_decay)\n",
    "    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "                          scope='dp1')\n",
    "    net = tf_util.fully_connected(net, 4, activation_fn=None, scope='fc3')\n",
    "\n",
    "    return net, end_points\n",
    "\n",
    "\n",
    "def get_loss(pred, label,end_points):\n",
    "    \"\"\" pred: B*NUM_CLASSES,\n",
    "        label: B, \"\"\"\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "    classify_loss = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('classify loss', classify_loss)\n",
    "    return classify_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gy9kouzzH-3I"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qxBY8zqNzBRJ"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "point_cloud = tf.compat.v1.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINTS, 3))\n",
    "label = tf.compat.v1.placeholder(tf.int32, shape=(BATCH_SIZE,))\n",
    "is_training = tf.constant(True)  # Set to True for training, False for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Tgc-7_S1_bQ6"
   },
   "outputs": [],
   "source": [
    "pred, end_points = get_model(point_cloud, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_xJT4np8F2Pw"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001  # Adjust this as needed\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(get_loss(pred, label, end_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MFxNW739GG9Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 19:29:39.169587: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-30 19:29:39.192538: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8DCgddzHmyC",
    "outputId": "b8944b5c-8fc8-426e-da7b-e8552460d843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 150, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOixp_JkGMbB",
    "outputId": "9cd3adf4-83da-4d74-b0dd-20a91f4fdd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.4361\n",
      "Validation Loss: 0.6659\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 2/50, Loss: 1.0080\n",
      "Validation Loss: 0.6489\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 3/50, Loss: 0.3373\n",
      "Validation Loss: 0.6802\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 4/50, Loss: 0.4574\n",
      "Validation Loss: 0.5335\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 5/50, Loss: 0.4977\n",
      "Validation Loss: 0.6773\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 6/50, Loss: 0.0586\n",
      "Validation Loss: 0.6558\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 7/50, Loss: 0.6139\n",
      "Validation Loss: 0.4505\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 8/50, Loss: 0.3291\n",
      "Validation Loss: 0.4436\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 9/50, Loss: 0.2707\n",
      "Validation Loss: 0.8942\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 10/50, Loss: 0.3590\n",
      "Validation Loss: 0.6033\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 11/50, Loss: 0.5320\n",
      "Validation Loss: 0.4322\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 12/50, Loss: 0.1926\n",
      "Validation Loss: 0.4213\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 13/50, Loss: 0.1738\n",
      "Validation Loss: 0.3471\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 14/50, Loss: 0.1110\n",
      "Validation Loss: 0.3451\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 15/50, Loss: 0.0277\n",
      "Validation Loss: 0.2948\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 16/50, Loss: 0.2242\n",
      "Validation Loss: 0.5135\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 17/50, Loss: 0.0440\n",
      "Validation Loss: 0.3986\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 18/50, Loss: 0.0063\n",
      "Validation Loss: 0.3498\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 19/50, Loss: 0.1136\n",
      "Validation Loss: 0.4314\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 20/50, Loss: 0.0330\n",
      "Validation Loss: 0.4520\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 21/50, Loss: 0.6341\n",
      "Validation Loss: 0.5441\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 22/50, Loss: 0.0221\n",
      "Validation Loss: 0.5644\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 23/50, Loss: 0.1855\n",
      "Validation Loss: 0.3389\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 24/50, Loss: 0.1154\n",
      "Validation Loss: 1.2161\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 25/50, Loss: 0.0721\n",
      "Validation Loss: 0.4031\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 26/50, Loss: 0.0135\n",
      "Validation Loss: 0.5407\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 27/50, Loss: 0.0173\n",
      "Validation Loss: 0.4138\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 28/50, Loss: 0.2034\n",
      "Validation Loss: 0.4725\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 29/50, Loss: 0.3817\n",
      "Validation Loss: 0.3050\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 30/50, Loss: 0.0113\n",
      "Validation Loss: 0.2021\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 31/50, Loss: 0.0120\n",
      "Validation Loss: 0.3801\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 32/50, Loss: 0.0079\n",
      "Validation Loss: 0.7484\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 33/50, Loss: 0.0031\n",
      "Validation Loss: 0.3762\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 34/50, Loss: 0.0044\n",
      "Validation Loss: 0.3990\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 35/50, Loss: 0.0281\n",
      "Validation Loss: 0.5106\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 36/50, Loss: 0.2422\n",
      "Validation Loss: 0.3939\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 37/50, Loss: 0.0020\n",
      "Validation Loss: 0.4369\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 38/50, Loss: 0.0050\n",
      "Validation Loss: 0.3727\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 39/50, Loss: 0.0095\n",
      "Validation Loss: 0.3016\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 40/50, Loss: 0.0034\n",
      "Validation Loss: 0.3206\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 41/50, Loss: 0.0017\n",
      "Validation Loss: 0.3252\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 42/50, Loss: 0.0219\n",
      "Validation Loss: 0.3867\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 43/50, Loss: 0.0025\n",
      "Validation Loss: 0.3099\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 44/50, Loss: 0.0007\n",
      "Validation Loss: 0.2741\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 45/50, Loss: 0.0002\n",
      "Validation Loss: 0.3084\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 46/50, Loss: 0.0023\n",
      "Validation Loss: 0.3088\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 47/50, Loss: 0.0004\n",
      "Validation Loss: 0.3026\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 48/50, Loss: 0.0011\n",
      "Validation Loss: 0.2849\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 49/50, Loss: 0.0005\n",
      "Validation Loss: 0.2891\n",
      "Saved model weights to model_weights/model.ckpt\n",
      "Epoch 50/50, Loss: 0.0035\n",
      "Validation Loss: 0.3044\n",
      "Saved model weights to model_weights/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.compat.v1.train.Saver()\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop through the training dataset in mini-batches\n",
    "    for i in range(0, len(X_train), BATCH_SIZE):\n",
    "        batch_data = X_train[i:i + BATCH_SIZE]\n",
    "        batch_labels = Y_train[i:i + BATCH_SIZE]\n",
    "\n",
    "        # Feed batch data into the model and run the training operation\n",
    "        feed_dict = {\n",
    "            point_cloud: batch_data,\n",
    "            label: batch_labels,\n",
    "            is_training: True  # Set to True for training\n",
    "        }\n",
    "        _, loss_val = sess.run([train_op, get_loss(pred, label, end_points)], feed_dict=feed_dict)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "    # Perform validation at the end of each epoch\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = len(X_test) // BATCH_SIZE\n",
    "    for j in range(0, len(X_test), BATCH_SIZE):\n",
    "        val_batch_data = X_test[j:j + BATCH_SIZE]\n",
    "        val_batch_labels = y_test[j:j + BATCH_SIZE]\n",
    "\n",
    "        feed_dict = {\n",
    "            point_cloud: val_batch_data,\n",
    "            label: val_batch_labels,\n",
    "            is_training: False  # Set to False for validation\n",
    "        }\n",
    "        val_loss += sess.run(get_loss(pred, label, end_points), feed_dict=feed_dict)\n",
    "\n",
    "    val_loss /= num_val_batches\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save the model's weights after each epoch (or at any desired frequency)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        save_path = saver.save(sess, \"model_weights/model.ckpt\")\n",
    "        print(f\"Saved model weights to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
