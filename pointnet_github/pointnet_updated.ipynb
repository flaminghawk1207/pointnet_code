{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0efa484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "import tf_util\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f5b892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n",
    "    labels_pl = tf.compat.v1.placeholder(tf.int32, shape=(batch_size))\n",
    "    return pointclouds_pl, labels_pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb2fa552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0]\n",
    "    num_point = point_cloud.get_shape()[1]\n",
    "    end_points = {}\n",
    "    input_image = tf.expand_dims(point_cloud, -1)\n",
    "    \n",
    "    # Point functions (MLP implemented as conv2d)\n",
    "    net = tf_util.conv2d(input_image, 64, [1,3],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "\n",
    "    # Symmetric function: max pooling\n",
    "    net = tf_util.max_pool2d(net, [num_point,1],\n",
    "                             padding='VALID', scope='maxpool')\n",
    "    \n",
    "    # MLP on global point cloud vector\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                  scope='fc1', bn_decay=bn_decay)\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                  scope='fc2', bn_decay=bn_decay)\n",
    "    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "                          scope='dp1')\n",
    "    net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')\n",
    "\n",
    "    return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c171630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(pred, label, end_points):\n",
    "    \"\"\" pred: B*NUM_CLASSES,\n",
    "        label: B, \"\"\"\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "    classify_loss = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('classify loss', classify_loss)\n",
    "    return classify_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3db3975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__=='__main__':\n",
    "#     with tf.Graph().as_default():\n",
    "#         inputs = tf.zeros((32,1024,3))\n",
    "#         outputs = get_model(inputs, tf.constant(True))\n",
    "#         print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66efde5",
   "metadata": {},
   "source": [
    "### Data Dealing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3321ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_directory = '/Users/anishayyagari/Documents/tester/pointnet implementation/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57f58d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38d45c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_percentage = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4436fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(dataset_directory):\n",
    "    folder_path = os.path.join(dataset_directory, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all CSV files in the folder\n",
    "        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "        \n",
    "        # Shuffle the list of CSV files\n",
    "        random.shuffle(csv_files)\n",
    "        \n",
    "        # Calculate the number of files to use for testing\n",
    "        num_test_samples = int(len(csv_files) * (test_split_percentage / 100.0))\n",
    "        \n",
    "        # Ensure that we have enough data for both train and test sets\n",
    "        if num_test_samples >= len(csv_files):\n",
    "            continue  # Skip this class if there's not enough data\n",
    "        \n",
    "        # Split the files into train and test\n",
    "        train_files = csv_files[num_test_samples:]\n",
    "        test_files = csv_files[:num_test_samples]\n",
    "        \n",
    "        # Load and append the data from each CSV file to the respective lists\n",
    "        for file_name in train_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            data = pd.read_csv(file_path)\n",
    "            train_data.append(data)\n",
    "            train_labels.append(folder)  # Append the folder name as the class label\n",
    "        \n",
    "        for file_name in test_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            data = pd.read_csv(file_path)\n",
    "            test_data.append(data)\n",
    "            test_labels.append(folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f785a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train tensor shape: (171183, 4)\n",
      "Y_train tensor shape: (293,)\n",
      "X_test tensor shape: (42109, 4)\n",
      "Y_test tensor shape: (71,)\n"
     ]
    }
   ],
   "source": [
    "if len(train_data) == 0 or len(test_data) == 0:\n",
    "    print(\"Not enough data for training or testing.\")\n",
    "else:\n",
    "    # Concatenate the data into train and test DataFrames\n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "    # Extract unique class labels\n",
    "    class_labels = list(set(train_labels + test_labels))\n",
    "    \n",
    "    # Convert class labels to numerical values\n",
    "    class_to_id = {label: i for i, label in enumerate(class_labels)}\n",
    "    train_labels = [class_to_id[label] for label in train_labels]\n",
    "    test_labels = [class_to_id[label] for label in test_labels]\n",
    "    train_df.to_csv('train_data.csv', index=False)\n",
    "    test_df.to_csv('test_data.csv', index=False)\n",
    "    X_train = train_df.to_numpy(dtype=np.float32)\n",
    "    Y_train = np.array(train_labels, dtype=np.int32)\n",
    "    X_test = test_df.to_numpy(dtype=np.float32)\n",
    "    Y_test = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "    # Convert NumPy arrays to TensorFlow tensors\n",
    "    X_train_tensor = tf.constant(X_train)\n",
    "    Y_train_tensor = tf.constant(Y_train)\n",
    "    X_test_tensor = tf.constant(X_test)\n",
    "    Y_test_tensor = tf.constant(Y_test)\n",
    "    print(\"X_train tensor shape:\", X_train_tensor.shape)\n",
    "    print(\"Y_train tensor shape:\", Y_train_tensor.shape)\n",
    "    print(\"X_test tensor shape:\", X_test_tensor.shape)\n",
    "    print(\"Y_test tensor shape:\", Y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db0276a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_points = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92cde9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ffd33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointclouds_pl, labels_pl = placeholder_inputs(batch_size, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "178cc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.constant(True)  # Set to True for training, False for inference\n",
    "pred, end_points = get_model(pointclouds_pl, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20a78551",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001  # Adjust this as needed\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(get_loss(pred, labels_pl, end_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c163959",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08627802",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # Adjust the number of epochs as needed\n",
    "num_batches = len(X_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26c66c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Start index: 0\n",
      "End index: 32\n",
      "Batch data shape: (32, 4)\n",
      "Batch labels shape: (32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (32, 4) for Tensor Placeholder_6:0, which has shape (32, 1024, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Feed batch data into the model and run the training operation\u001b[39;00m\n\u001b[1;32m     27\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     pointclouds_pl: batch_data,\n\u001b[1;32m     29\u001b[0m     labels_pl: batch_labels,\n\u001b[1;32m     30\u001b[0m     is_training: \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set to True for training\u001b[39;00m\n\u001b[1;32m     31\u001b[0m }\n\u001b[0;32m---> 32\u001b[0m _, loss_val \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_pl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Print loss for monitoring\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/client/session.py:1165\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1161\u001b[0m   np_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(subfeed_val, dtype\u001b[38;5;241m=\u001b[39msubfeed_dtype)\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_tensor_handle_feed \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m subfeed_t\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39mis_compatible_with(np_val\u001b[38;5;241m.\u001b[39mshape)):\n\u001b[0;32m-> 1165\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1166\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot feed value of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(np_val\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for Tensor \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1167\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfeed_t\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which has shape \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1168\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(subfeed_t\u001b[38;5;241m.\u001b[39mget_shape())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mis_feedable(subfeed_t):\n\u001b[1;32m   1170\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfeed_t\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m may not be fed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (32, 4) for Tensor Placeholder_6:0, which has shape (32, 1024, 4)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data at the start of each epoch\n",
    "#     shuffled_indices = np.arange(len(X_train))\n",
    "#     np.random.shuffle(shuffled_indices)\n",
    "#     X_train_shuffled = X_train[shuffled_indices]\n",
    "#     Y_train_shuffled = Y_train[shuffled_indices]\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "        print(\"Batch:\", batch_idx)\n",
    "        print(\"Start index:\", start_idx)\n",
    "        print(\"End index:\", end_idx)\n",
    "\n",
    "        # Ensure that end_idx does not exceed the size of your data\n",
    "        if end_idx > len(X_train):\n",
    "            end_idx = len(X_train)\n",
    "\n",
    "        batch_data = X_train[start_idx:end_idx]\n",
    "        batch_labels = Y_train[start_idx:end_idx]\n",
    "\n",
    "        print(\"Batch data shape:\", batch_data.shape)\n",
    "        print(\"Batch labels shape:\", batch_labels.shape)\n",
    "\n",
    "        # Feed batch data into the model and run the training operation\n",
    "        feed_dict = {\n",
    "            pointclouds_pl: batch_data,\n",
    "            labels_pl: batch_labels,\n",
    "            is_training: True  # Set to True for training\n",
    "        }\n",
    "        _, loss_val = sess.run([train_op, get_loss(pred, labels_pl, end_points)], feed_dict=feed_dict)\n",
    "\n",
    "        # Print loss for monitoring\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{num_batches}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "# Training complete\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Optionally, save the trained model\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.save(sess, 'pointnet_model')\n",
    "\n",
    "# Don't forget to close the session when done\n",
    "sess.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73b5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
